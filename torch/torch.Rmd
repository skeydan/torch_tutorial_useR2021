---
title: "Getting started with torch"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(torch)
library(luz)
library(torchvision)
library(dplyr)
knitr::opts_chunk$set(echo = FALSE)
```

## Welcome to `torch`! (And `luz`!)

### `torch`

[`torch`](http:/torch.mlverse.org) is an *R-native* framework for fast array computation with automatic differentiation and rich neural-network functionality.

TBD pic

### `luz`

What `keras` is to `tensorflow` -- a high-level API streamlining and instrumenting the training process -- `luz` is to `torch`. While everything can be accomplished with `torch` alone, `luz`

-   provides a *keras*-like, declarative interface to the training process

-   removes boilerplate code

-   keeps track of *metrics* commonly used in data science, and allows you to define your own

-   provides a set of ready-to-use *callbacks* to control training, save model state, and more

-   offers a convenient interface for defining your own callbacks

### Ecosystem

-   [torch](https:/github.com/mlverse/torch)

-   [luz](https:/github.com/mlverse/luz)

-   [torchvision](https:/github.com/mlverse/torchvision)

-   [torchdatasets](https:/github.com/mlverse/torchdatasets)

-   [tabnet](https:/github.com/mlverse/tabnet)

-   ... and more!

### Our goals for today

1.  Understand and use `torch` tensors and neural network modules; understand and apply automatic differentiation.

2.  Use `luz` to train neural networks in a declarative way.

3.  Get started with time-series forecasting in `torch`.

### Prerequisites

To follow this tutorial, you will need to install the following packages:

```{r}
library(torch)
library(luz)
library(torchvision)

library(learnr)
library(dplyr)
```

## `torch` tensors, modules, and autograd

### Tensors

#### Creating tensors

##### Way 1: From R values

Tensors can be created directly from R values using `torch_tensor()`. Optionally, we can define tensor *attributes*, including the data type, the device it lives on, and more.

Here we are creating one-dimensional tensors (vectors):

```{r, eval=FALSE, echo=TRUE}
torch_tensor(1)
torch_tensor(1, dtype = torch_int())
torch_tensor(1, device = "cuda")

torch_tensor(c(1, 2, 3)) # float tensor
```

Two-dimensional tensors can be created from R matrices.

```{r, eval=FALSE, echo=TRUE}
torch_tensor(matrix(1:9, ncol = 3)) # integer tensor
torch_tensor(matrix(1:9, ncol = 3))$to(dtype = torch_float()) # cast to float

torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))
```

Higher-dimensional tensors can be created from R arrays, but normally it is easier to use bulk creation functions.

##### Way 2: Bulk creation functions.

Multi-dimensional tensors following some defineable pattern are created passing in the desired dimensionality. A few examples (more exist):

```{r, eval=FALSE, echo=TRUE}
torch_zeros(c(3, 3))
torch_rand(c(3, 3))

```

Another often-used type of function is used specifying the desired range:

```{r, eval=FALSE, echo=TRUE}
torch_arange(1, 9)
torch_logspace(start = 0.1, end = 1.0, steps = 5)
```

#### Converting back to R

Tensors are converted back to R using `as.numeric()`, `as.matrix()`, or `as.array()`:

```{r, eval=FALSE, echo=TRUE}
torch_tensor(2) %>% as.numeric()

torch_ones(c(2, 2)) %>% as.matrix() 

torch_ones(c(2, 2, 2)) %>% as.array() 
```

#### Operations on tensors

A great number of operations can be performed on tensors. In general, there is a pair of *function* (not associated to any object) and corresponding *method* ("belonging" to a tensor instance) that do the same thing:

```{r, eval=FALSE, echo=TRUE}
t1 <- torch_tensor(c(1, 2, 3))
t2 <- torch_tensor(c(1, 2, 3))

torch_add(t1, t2)
t1$add(t2)
```

In both cases, the original tensors are not modified; a new object is created. Normally, you would just assign this to a new variable:

```{r, eval=FALSE, echo=TRUE}
t3 <- t1$add(t2)

t1
t3
```

In the few cases where you need to modify the original tensor, you can make use of the corresponding underscore variants:

```{r, eval=FALSE, echo=TRUE}
t1$add_(t2)
t1
```

Here are a few of the many matrix operations available. `$mul()` multiplies element-wise; `$matmul()` performs matrix multiplication; `$dot()` computes the dot product:

```{r, eval=FALSE, echo=TRUE}
t1$mul(t2)

# both work (torch has no concept of row vs. column vector)
t1$matmul(t2)
t1$t()$matmul(t2)

t1$dot(t2)
```

You can see that `torch` makes no distinction between row and column vectors. Above, `$t()` transposes the vector `t1`, but the matrix multiplication will work without.

#### Reshaping tensors

Often, you will need to reshape a tensor. Among the most common operations are `$squeeze()` and `$unsqueeze()`. The former adds removes a singleton dimension at the specified position (where singleton means the dimension is of length `1`):

```{r, eval=FALSE, echo=TRUE}
t1 <- torch_randn(c(1, 2, 3, 4))
t1

t1$squeeze(1)
```

The latter, in contrast, adds a singleton dimension:

```{r, eval=FALSE, echo=TRUE}
t1$unsqueeze(4)
```

This only works for singleton dimensions. `$view()` works for arbitrary reshaping, provided the number of elements allows for it. `t1` , above, has 24 values, which could as well be arranged as 6x4 or 1x24:

```{r, eval=FALSE, echo=TRUE}
t1$view(c(6, 4))

t1$view(24)

```

`$view()` does not actually create a separate, re-shaped instance of its input tensor; instead, it has the new variable refer to the same location in memory, and just stores some metadata that tell `torch` how the respective bytes should be read. There are cases when `$view()` cannot be used; in this case, you can always use `$reshape()` instead. In contrast to `$view()`, `$reshape()` will make a physical copy if necessary.

#### Indexing and slicing

Indexing in `torch` is 1-based, just like in R overall. And just like in R, singleton dimensions will be dropped -- unless you specify `drop = FALSE`:

```{r, eval=FALSE, echo=TRUE}
t1

t1[ , 1, , ]
t1[ , 1, , , drop = FALSE]

```

Ranges of values ("slices") can be accessed using the semicolon:

```{r, eval=FALSE, echo=TRUE}
t1[1, 1, 1:2, ]
t1[1, 1, 1:2, , drop = FALSE]
```

A shortcut that does not exist in R (where the same syntax has different semantics), index `-1` is used to refer to the last element in a dimension:

```{r, eval=FALSE, echo=TRUE}
t2 <- torch_tensor(1:17)
t2[-1] 
```

#### Broadcasting

In `torch`, tensors may be *broadcasted*. The principle is the same as when, in R, we add a scalar to every element in a vector. But it goes farther than that. We don't have the time to explain the rules in detail, but show a few examples as well as state the rules, for you to return at a later time.

Here, we "add" a matrix and a vector, resulting in the vector being added to every *row* of the matrix. This is possible only because `t2` has a singleton dimension in front.

```{r, eval=FALSE, echo=TRUE}
t1 <- torch_randn(c(3,5))
t2 <- torch_randn(c(1,5))

t1$add(t2)
```

This example looks similar, but it involves an additional operation from `torch`'s side: `t2` is first virtually expanded to size 1x5 (a singleton dimension is added in front). Then, things go like above.

```{r, eval=FALSE, echo=TRUE}
t1 <- torch_randn(c(3,5))
t2 <- torch_randn(c(5))

t1$add(t2)
```

As a final example, here we see both virtual addition of a singleton dimension (to `t1`) and the "reusability" of singleton dimensions shown in the first example. The latter idea is used twice, for `t1` as well as `t2`.

#### Appendix: Broadcasting rules

    # 1 We align array shapes, starting from the right.
      
      # Example

      # t1, shape:     8  1  6  1
      # t2, shape:        7  1  5
      

    # 2 Starting to look from the right, the sizes along aligned axes either have to match exactly,
    #   or one of them has to be equal to 1.
    #   In the latter case, the 1-dimensional tensor is broadcast to the larger one.

      # Example: this happens in the last (for t1) as well as the second-from-last dimension (for t2)

      # t1, shape:     8  1  6  5
      # t2, shape:        7  6  5


    # 3 If on the left, one of the arrays has an additional axis (or more than one),
    #   the other is virtually expanded to have a size of 1 in that place.
    #   Then, broadcasting will happen as stated in (2).

      # Example: this happens in t1â€™s leftmost dimension. First, there is a virtual expansion

      # t1, shape:     8  1  6  1
      # t2, shape:     1  7  1  5

      # and then, broadcasting happens:
      
      # t1, shape:     8  1  6  1
      # t2, shape:     8  7  1  5

#### Exercise: Tensors

In the following exercises, try translating the R code into equivalent operations using `torch`.

1.  Create two tensors representing a matrix and a vector, respectively:

```{r tensors1, exercise=TRUE, exercise.eval=TRUE}
# a matrix
m1 <- matrix(1:32, ncol = 8, byrow = TRUE)

# really a vector
m2 <- matrix(1:8, ncol = 1)

m1
m2
```

```{r tensors1-hint}
t1 <- torch_tensor(matrix(1:32, ncol = 8, byrow = TRUE))
t2 <- torch_tensor(1:8)

t1
t2
```

2.  Multiply the matrices, sum over all elements, and take the square root:

```{r tensors2, exercise=TRUE, exercise.eval=TRUE}
(m1 %*% m2)^2 %>% sum() %>% sqrt()
```

```{r tensors2-hint}
t1$matmul(t2)$square()$sum()$to(dtype = torch_float())$sqrt()
```

[Note how we need to cast to `float` in order to be able to call `torch_sqrt()`.]

3.  Multiply each row in `m1` by the vector `m2` (element-wise):

```{r tensors3, exercise=TRUE, exercise.eval=TRUE}
m1 * rbind(t(m2), t(m2), t(m2), t(m2))
```

```{r tensors3-hint}
t1 * t2
```

[Note how broadcasting takes care of the duplication for us. Also, note how no transposition is needed, as `torch` has no concept of row vectors vs. column vectors.]

4.  Transpose the matrix `m1`, and compute column sums. (This should yield 4 values.)

```{r tensors4, exercise=TRUE, exercise.eval=TRUE}
t(m1) %>% apply(2, sum)
```

```{r tensors4-hint}

t1$t()$sum(dim = 1)
```

[Note how applying the sum over dimension 1 (not 2) collapses the rows. Try to view it like this: Given an index into the dimensions, in R, we think "group by". In torch, we think "collapse".]

5.  Standardize `m1` , subtracting the mean and dividing by the standard deviation.

```{r tensors5, exercise=TRUE, exercise.eval=TRUE}
(m1 - mean(m1)) / sd(m1)
```

```{r tensors5-hint}
t1 <- t1$to(dtype = torch_float())
(t1 - t1$mean()) / t1$std()
```

Just like `torch_sum()`, `torch_mean()` and `torch_std()` need their input to be of type `float`.

### Automatic differentiation with *autograd*

#### How it works

`torch` autograd provides automatic differentiation for operations executed on tensors. For this to happen, the "source" (or "leaf", as `torch` calls it) tensor -- the one *with respect to which* we'd like derivatives computed -- needs to be created with `requires_grad = TRUE`. Let's call it `a`:

```{r, eval=FALSE, echo=TRUE}
a <- torch_tensor(matrix(1:4, ncol = 2, byrow = TRUE), dtype = torch_float(), requires_grad = TRUE)

```

In this example, `c`, the output, depends on `a` via `b`:

```{r, eval=FALSE, echo=TRUE}
b <- a$mul(2)
c <- b$sum()
```

So far, no derivatives have been computed yet. But `torch` knows what to do should we ask it to. More precisely, it knows the concrete operations it'll have to compute the derivatives for:

```{r, eval=FALSE, echo=TRUE}
c$grad_fn
b$grad_fn
```

To actually have them computed, call `$backward()` on the output tensor:

```{r, eval=FALSE, echo=TRUE}
c$backward()
```

Now the gradient of `c` with respect to `a` can be found in `a`'s `$grad` field.

```{r, eval=FALSE, echo=TRUE}
a$grad
```

When we're updating a "leaf" tensor, for example in optimization, we don't want `torch` to record that operation for later computation of derivatives. In these cases, we need to tell it to exempt the operation in question from the process:

```{r, eval=FALSE, echo=TRUE}
     
with_no_grad( {
  a$sub_(0.1 * a$grad)
})

a
```

#### Minimizing a function with *autograd*

We can use *autograd* to minimize a function. We define a parameter to hold $\mathbf{x}$. Then, in a loop, we evaluate the function at the current $\mathbf{x}$, compute the gradient, and subtract a fraction of the gradient from $\mathbf{x}$.

```{r, eval=FALSE, echo=TRUE}

# function to minimize
f <- function(x) x^2 - 7

# we start from x = 11
param <- torch_tensor(11, requires_grad = TRUE)

# learning rate: fraction of gradient to subtract
lr <- 0.1

for (i in 1:num_iterations) {
  
  # call function on current parameter value

  # compute gradient of value w.r.t. parameter

  # update parameter

}
```

In the exercise, you're asked to fill in the missing pieces.

#### Exercise: Function minimization

Fill in the lines marked "TBD". When you have the code running, experiment with the learning rate and compare the results. What is a good learning rate for this problem?

```{r autograd, exercise=TRUE, exercise.eval=TRUE}
# function to minimize
f <- function(x) x^2 - 7

# we start from x = 11
param <- torch_tensor(11, requires_grad = TRUE)

# learning rate: fraction of gradient to subtract
lr <- 0.1

for (i in 1:10) {
  
  cat("Iteration: ", i, "\n")
  
  # call function with current parameter
  value <- 777 # TBD
  cat("Value is: ", as.numeric(value), "\n")
  
  # compute gradient of value w.r.t. parameter
  # TBD
  # uncomment the followig line when ready
  # cat("Gradient is: ", as.matrix(param$grad), "\n")
  
  # update parameter
  # wrap in with_no_grad
  with_no_grad({
    # subtract a fraction of gradient from param
    # TBD
    
    # zero out on every iteration (would accumulate otherwise)
    # TBD
  })
  
  cat("After update: Param is: ", as.matrix(param), "\n\n")
  
  if (abs(-7 - as.numeric(value)) < 0.00005) break
}
```

```{r autograd-hint}
# function to minimize
f <- function(x) x^2 - 7

# we start from x = 11
param <- torch_tensor(11, requires_grad = TRUE)

# learning rate: fraction of gradient to subtract
lr <- 0.5

for (i in 1:10) {
  
  cat("Iteration: ", i, "\n")
  
  value <- f(param)
  cat("Value is: ", as.numeric(value), "\n")
  
  # compute gradient of value w.r.t. parameter
  value$backward()
  cat("Gradient is: ", as.matrix(param$grad), "\n")
  
  # update
  with_no_grad({
    param$sub_(lr * param$grad)
    # zero out on every iteration (would accumulate otherwise)
    param$grad$zero_()
  })
  
  cat("After update: Param is: ", as.matrix(param), "\n\n")
  
  if (abs(-7 - as.numeric(value)) < 0.00005) break
}

```

### Modules and optimizers

While everything can be done with tensors and *autograd* alone, coding a large neural network that way would be a pretty cumbersome task. Luckily, doing so is not necessary. For one, `torch` provides a rich set of neural network *modules* that hide away layer logic; and secondly, its *optimizers* encapsulate established optimization algorithms known for their efficiency in neural-network settings.

#### Neural network modules

`torch` uses the term module for individual *layers* (e.g., linear layer, convolutional layer ...) as well as *models*, a.k.a. neural networks. The logic here is that modules are composable; a model/module is nothing but a composition of smaller modules, which again may contain yet smaller modules, etc.

##### Linear module

Here is an affine transformation, coded manually:

```{r, eval=FALSE, echo=TRUE}
# input data
x <- torch_randn(c(7,2))

# weights
w <- torch_tensor(c(0.1, 0.1), requires_grad = TRUE)
# bias
b <- torch_tensor(0.5, requires_grad = TRUE)
  
x$matmul(w) + b  
```

We can achieve the same using a linear module:

```{r, eval=FALSE, echo=TRUE}
l <- nn_linear(in_features = 2, out_features = 1)
l(x)
```

The result is different from above, because there we defined the weight ourselves. By default, `torch` will initialize the weights uniformly, with values ranging between `[-sqrt(num_features), sqrt(num_features)]`.

```{r, eval=FALSE, echo=TRUE}
l$weight 
```

Just to prove the point, we can manually initialize the module's weights:

```{r, eval=FALSE, echo=TRUE}
nn_init_constant_(l$weight, 0.1)
nn_init_constant_(l$bias, 0.5)

l(x)
```

With modules, we get automatic differentiation for free.

Assume we want to minimize the sum of the outputs.

```{r, eval=FALSE, echo=TRUE}
loss <- l(x)$sum() 
loss$grad_fn
```

We will still have to call `$backward()` to see actual derivatives being computed. Here they are still undefined:

```{r, eval=FALSE, echo=TRUE}
l$weight$grad
l$bias$grad
```

Calling `$backward()` ...

```{r, eval=FALSE, echo=TRUE}
loss$backward()

l$weight$grad
l$bias$grad
```

##### Examples of other modules

Many more modules exist. Here is a tensor mimicking a 32x32 RGB image:

```{r, eval=FALSE, echo=TRUE}
img <- torch_rand(c(1, 3, 32, 32))
```

Now `nn_conv2d()` is used to create a convolutional layer, and its 3x3 filter is applied to the image:

```{r, eval=FALSE, echo=TRUE}
conv <- nn_conv2d(in_channels = 3, out_channels = 1, kernel_size = 3, padding = 1)

conv(img)
```

Another module commonly used in image processing is `nn_max_pool2d()` for spatial downsizing:

```{r, eval=FALSE, echo=TRUE}
pool <- nn_max_pool2d(kernel_size = 2)
conv(img) %>% pool()
```

In the third section of this tutorial, we will encounter modules common in time series processing.

##### Composing modules

To build a "model" from "layers" such as the ones we showed above, we can use `nn_sequential()`. Here is a model that has two linear layers, with between them a ReLU module (`nn_relu()`). ReLU stands for "Rectified Linear Unit"; its purpose is to introduce some nonlinearity to this otherwise linear model. (It does this by setting all negative values to zero.)

```{r, eval=FALSE, echo=TRUE}
model <- nn_sequential(
  nn_linear(2, 16),
  nn_relu(),
  nn_linear(16, 1)
)

model$parameters

model(x)
```

You can also define your own modules. We'll see examples of this in the second part.

#### Optimizers

Among the most commonly-used optimizers in deep learning are Adam (`optim_adam()`), RMSProp (`optim_rmsprop()`), and Stochastic Gradient Descent (SGD; `optim_sgd()`).

Here, we use `optim_adam()` to demonstrate their use.

```{r, eval=FALSE, echo=TRUE}
# some toy data
x <- torch_tensor(c(1.2, 0.8, 0.7))
y <- torch_tensor(1)
```

When an optimizer object is created, it needs to be told what to optimize -- namely, the model's parameters. Most optimizers also need to be passed the learning rate.

```{r, eval=FALSE, echo=TRUE}
model <- nn_sequential(
  nn_linear(3, 8),
  nn_relu(),
  nn_linear(8, 1)
)

optimizer <- optim_adam(model$parameters, lr = 0.01)
```

We obtain a prediction:

```{r, eval=FALSE, echo=TRUE}
prediction <- model(x)
prediction
```

We then use one of `torch`'s built-in loss functions to compute the loss (here, mean squared error):

```{r, eval=FALSE, echo=TRUE}
loss <- nnf_mse_loss(prediction, y)
loss
```

We call `$backward()` on the loss to have gradients computed:

```{r, eval=FALSE, echo=TRUE}
loss$backward()
```

Now the gradients are known, but no changes have been made to the model's parameters yet.

```{r, eval=FALSE, echo=TRUE}
model$parameters
```

Calling `$step()` on the optimizer will make those changes:

```{r, eval=FALSE, echo=TRUE}
optimizer$step()
```

```{r, eval=FALSE, echo=TRUE}
model$parameters
```

When actually training a network, we call the optimizer in a loop. We show an example of this next.

##### A neural network

First, we create the training data.

```{r, eval=FALSE, echo=TRUE}
# input dimensionality (number of input features)
d_in <- 3
# output dimensionality (number of predicted features)
d_out <- 1
# number of observations in training set
n <- 100

# create random data
x <- torch_randn(n, d_in)
y <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)

```

Then, we define the network.

```{r, eval=FALSE, echo=TRUE}
# dimensionality of hidden layer
d_hidden <- 32

model <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

```

We create the optimizer:

```{r, eval=FALSE, echo=TRUE}
learning_rate <- 0.08

# optimizer applies gradient updates for us
optimizer <- optim_adam(model$parameters, lr = learning_rate)
```

And we 're ready for the training loop. In a loop, we

-   obtain model predictions;

-   compute the loss;

-   propagate back the loss through the network and update the parameters.

Note how when optimizing in a loop, we need to zero out gradients on every iteration.

```{r, eval=FALSE, echo=TRUE}
for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  y_pred <- model(x)
  
  ### -------- compute loss -------- 
  # mean squared error loss
  loss <- nnf_mse_loss(y_pred, y, reduction = "sum")
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation -------- 
  
  # Need to zero out the gradients before the backward pass, as they'd accumulate otherwise
  optimizer$zero_grad()
  
  # compute gradients 
  loss$backward()
  
  # update weights 
  optimizer$step()
}
```

While a lot more convenient than working with tensors only, this is still a pretty low-level way of training a neural network, and needs special attention to be devoted to particularities (like zeroing out the gradients). In part two, we'll see how to train neural networks much more comfortably with `luz`.

#### Exercise: Training a neural network

Below, you find the above end-to-end code to train a neural network. Try experimenting with it a bit:

-   If you change the learning rate, what happens?

-   Try other optimizers, such as `optim_sgd()`. How does that affect training?

```{r modules, exercise=TRUE, exercise.eval=TRUE}
# input dimensionality (number of input features)
d_in <- 3
# output dimensionality (number of predicted features)
d_out <- 1
# number of observations in training set
n <- 100

# create random data
x <- torch_randn(n, d_in)
y <- x[, 1, NULL] * 0.2 - x[, 2, NULL] * 1.3 - x[, 3, NULL] * 0.5 + torch_randn(n, 1)

# dimensionality of hidden layer
d_hidden <- 32

model <- nn_sequential(
  nn_linear(d_in, d_hidden),
  nn_relu(),
  nn_linear(d_hidden, d_out)
)

learning_rate <- 0.08

# optimizer applies gradient updates for us
optimizer <- optim_adam(model$parameters, lr = learning_rate)

for (t in 1:200) {
  
  ### -------- Forward pass -------- 
  y_pred <- model(x)
  
  ### -------- compute loss -------- 
  # mean squared error loss
  loss <- nnf_mse_loss(y_pred, y, reduction = "sum")
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  ### -------- Backpropagation -------- 
  
  # Need to zero out the gradients before the backward pass, as they'd accumulate otherwise
  optimizer$zero_grad()
  
  # compute gradients 
  loss$backward()
  
  # update weights 
  optimizer$step()
}
```

## Training neural networks with `luz`

`luz` is a high-level API for `torch` that allows you to train neural networks in a declarative style.

With `luz`, the overall flow looks a lot like in `keras`:

1.  You define a model.

2.  You use `setup()` to configure it with a loss function, an optimizer, and a set of metrics.

3.  You train it using `fit()`, passing in the training and (optionally) validation data, as well as the number of epochs to train for and a set of callbacks (both optional).

### End-to-end example: MNIST 

#### Data

In `torch` , data is fed to a network using `dataset`s and `dataloaders`. Their respective responsibilities are:

-   `dataset`: Return a single training (or validation, or test) item (in supervised learning, a list of input and target). Optionally, take care of any pre-processing required.

-   `dataloader`: Feed the data to the model. Normally, this happens in *batches* of configurable size. Optionally, a `dataloader` may shuffle the data, and arrange for parallelization over a subset of available processors.

Here, we make use of the famou MNIST `dataset` provided by `torchvision` .

The dataset will be downloaded and prepared the first time it is requested (unless it exists in the specified location already). Below, this happens when creating the training set. When we instantiate the validation set, no download will take place.

We use `transform =` to tell `torch` how the images should be pre-processed.

```{r}
dir <- "~/Downloads/mnist" 

train_ds <- mnist_dataset(
  root = dir,
  train = TRUE,
  transform = function(x) {
    x %>%
      transform_to_tensor()
  },
  download = TRUE
)

valid_ds <- mnist_dataset(
  dir,
  train = FALSE,
  transform = function(x) {
    x %>%
      transform_to_tensor()
  }
)

```

We can query `dataset`s for their length:

```{r}
length(train_ds)
length(valid_ds)
```

Now, we create the respective `dataloader`s.

```{r}
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
valid_dl <- dataloader(valid_ds, batch_size = 32, shuffle = FALSE)
```

With dataloaders, `length()` indicates the number of *batches*:

```{r}
length(train_dl)
length(valid_dl)
```

#### Model

To speed up training, we make use of a pre-trained model that comes with `torchvision` (namely, AlexNet).

Here you see a top-level model that does not use the `nn_sequential()` constructor. This is because, even though we *are* chaining two submodules (the pre-trained model, and a sequential module consisting of a bunch of linear layers), there is one extra action we want to take: exempt the pre-trained model's weights from backpropagation.

The idea here is: make use of a model that already has been trained for thousands of GPU hours; then, adapt it for our specific task. This is called transfer learning. We just need to make sure that the final linear layer has its number of output features set to the number of different classes present *in our task.* In MNIST, there are ten classes.

```{r}
net <- nn_module(
  
  initialize = function(num_classes) {
    self$model <- model_alexnet(pretrained = TRUE)

    for (par in self$parameters) {
      par$requires_grad_(FALSE)
    }

    self$model$classifier <- nn_sequential(
      nn_dropout(0.5),
      nn_linear(9216, 256),
      nn_relu(),
      nn_linear(256, num_classes)
    )
  },
  forward = function(x) {
    self$model(x)[,1]
  }
)
```

```{r}

```
